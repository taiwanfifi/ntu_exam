# 附錄：機率教室 — 概念總覽

> 本附錄為《機率預言師》的教學補充材料。每個章節提到的機率統計概念，都在這裡有完整的數學推導、圖示、以及可以動手算的數值範例。

---

## 使用方式

1. 讀完小說章節後，翻到對應的教學頁面
2. 看公式推導——理解「為什麼」
3. 看圖表——直覺理解分布的「形狀」
4. 動手算數值範例——用小說裡的實際數字驗算
5. 回到小說——你會發現角色說的每一句話都有數學根據

---

## 全書概念地圖

### 第一篇：校準（第1-12章）→ [01_校準篇.md](01_校準篇_貝氏基礎.md)

| 章節 | 核心概念 | 關鍵數字 |
|------|----------|----------|
| 第1章 透鏡日常 | Beta 分布、期望值、標準差 | Beta(6.2, 3.8), E=0.62 |
| 第2章 校準悲傷的人 | 認知偏移、先驗偏差 | +15% 偏移 |
| 第3章 白奶奶的先驗 | 先驗信念、不確定性 | 四十年經驗 |
| 第4章 十一秒 | Poisson 分布、條件機率、貝氏更新 | P(A\|B) vs P(B\|A) |
| 第5章 太整齊了 | Poisson 擬合、MLE、偏態係數 | λ=1.67, skew=0.02 |
| 第6章 模式 | 異常偵測、人工 vs 自動校準 | 大安區 0.02 vs 萬華 0.71 |
| 第7章 似然函數 | 似然函數、MLE、似然比 | MLE=2.63 vs 官方 1.8 |
| 第8-9章 路燈下、似然比 | 似然比檢定、p-value | LR=4.72, p<0.001 |
| 第10章 不是Bug | 先驗替換、貝氏更新機制 | E值從 2.55 跳到 1.00 |
| 第11章 父親的定理 | 貝氏定理完整公式 | P(θ\|D) ∝ P(θ)×P(D\|θ) |
| 第12章 你發現了對不對 | 系統架構、加密分析 | 1,247次訪問 |

### 第二篇：雜訊（第13-24章）→ [02_雜訊篇.md](02_雜訊篇_分布與更新.md)

| 章節 | 核心概念 | 關鍵數字 |
|------|----------|----------|
| 第14章 分布的味道 | 四大分布家族 | Normal, Exp, Gamma, Beta |
| 第15章 馬可瑜的數據 | 重尾分布、相關係數 | r = 0.89 |
| 第16章 共軛先驗 | Beta-Binomial 共軛 | Beta(α+k, β+n-k) |
| 第20章 714 | Beta(72,28) 密碼 | 72/100 = 0.72 |
| 第21章 六個字元 | 參數壓縮、方差 | Var = 0.002 |
| 第22章 聯合與邊際 | 聯合分布、邊際分布 | f(x,y), ∫f(x,y)dy |
| 第23章 時間窗口 | 核密度估計 (KDE) | 三峰：8月/11月/2月 |

### 第三篇：信號（第25-36章）→ [03_信號篇.md](03_信號篇_期望值與不等式.md)

| 章節 | 核心概念 | 關鍵數字 |
|------|----------|----------|
| 第25章 蔥油餅會議 | 期望值、線性性 | E[aX+b] = aE[X]+b |
| 第26章 期望值與代價 | LOTUS 定理 | 480億損失 |
| 第27章 變異數與分歧 | 變異數、標準差 | σ=0.15 |
| 第28章 大數法則的幽靈 | LLN、CLT | √n(X̄-μ)/σ → N(0,1) |
| 第29章 不等式的鐵壁 | Markov、Chebyshev 不等式 | k=5 → ≤4% |
| 第30章 統計蜜罐 | Gamma 分布、5σ 門檻 | Gamma(3.2, 1.1) |
| 第33章 凌晨兩點三十七分 | 5.23σ 觸發 | |Δ|=1.62, 5.23σ |
| 第34章 動差生成函數 | MGF、唯一性定理 | M_A(t) ≠ M_B(t) |

### 第四篇：檢定（第37-48章）→ [04_檢定篇.md](04_檢定篇_假設檢定.md)

| 章節 | 核心概念 | 關鍵數字 |
|------|----------|----------|
| 第37-38章 起訴書、顯著水準 | H₀ vs H₁、α 水準 | α = 0.001 |
| 第39章 開庭 | 假設檢定流程 | P(正面)=0.8 |
| 第40章 百分之五的冤枉 | Type I Error | α = 0.001 |
| 第41章 百分之二十三的沉默 | Type II Error、Power | β=0.23, Power=0.77 |
| 第42章 正在消失的數字 | 信賴區間、樣本量 | n: 347→329 |
| 第43章 保險箱 | 獨立數據、精確度 | n = 6,083 |
| 第44章 卡方 | χ² 檢定 | χ²=47.3, df=8 |
| 第45-46章 裁定、漣漪 | 決策後果、社會衝擊 | 恐懼 38% |
| 第47章 三十年前的論文 | P(free will)≠0 | 先驗倫理 |

### 第五篇：隨機（第49-60章）→ [05_隨機篇.md](05_隨機篇_馬可夫與漫步.md)

| 章節 | 核心概念 | 關鍵數字 |
|------|----------|----------|
| 第49章 第二階段 | 馬可夫鏈、轉移矩陣 | 152 states, π=πP |
| 第50章 七十二小時 | 遍歷性、收斂速率 | \|λ₂\|=0.847 |
| 第51章 擲硬幣 | 隨機漫步、偏幣 | 55% vs 45% |
| 第52章 收斂 | 二維漫步、效率 | 效率 51.4% |
| 第53章 Poisson的意外 | Poisson 過程、指數分布 | λ=0.58, E[T]=1.72hr |
| 第55章 天台 | 過度擬合、黑天鵝 | P=0.000003 |
| 第56章 S.L.H. | Graceful shutdown | 23,147,882 個透鏡 |
| 第57章 選擇 | 自由選擇分布 | 33.1% / 29.4% / 16.8% |
| 第59-60章 隨機、蔥花 | 後驗 ∝ 先驗 × 似然 | P(蔥花香) = 1 |

---

## 圖表目錄

所有圖表位於 `圖表/` 資料夾，由 `generate_plots.py` 生成。

| 編號 | 檔名 | 對應概念 |
|------|------|----------|
| 01 | beta_distribution.png | Beta 分布家族 + Beta(72,28) |
| 02 | bayesian_update.png | 貝氏更新三步驟 |
| 03 | sequential_bayesian.png | 逐步貝氏更新收斂 |
| 04 | likelihood_function.png | 似然函數與 MLE |
| 05 | poisson_distribution.png | Poisson 分布 + λ=0.58 |
| 06 | distribution_families.png | Normal/Exp/Gamma/Beta 四大家族 |
| 07 | central_limit_theorem.png | 中央極限定理視覺化 |
| 08 | conjugate_prior.png | Beta-Binomial 共軛先驗 |
| 09 | joint_marginal.png | 聯合分布與邊際分布 |
| 10 | expected_value.png | 離散/連續期望值 |
| 11 | variance.png | 變異數與 68-95-99.7 法則 |
| 12 | law_of_large_numbers.png | 大數法則收斂 |
| 13 | chebyshev_inequality.png | Chebyshev 不等式 + 5σ 蜜罐 |
| 14 | mgf.png | 動差生成函數 + Gamma MGF 對比 |
| 15 | hypothesis_testing.png | H₀ vs H₁ + p-value |
| 16 | type_errors.png | Type I/II 決策矩陣 + Power curve |
| 17 | chi_square.png | χ² 分布 + χ²=47.3 檢定 |
| 18 | confidence_interval.png | 信賴區間 vs 樣本量 |
| 19 | markov_chain.png | 馬可夫鏈收斂 + |λ₂| 速率 |
| 20 | random_walk.png | 公平 vs 偏幣隨機漫步 |
| 21 | 2d_random_walk.png | 二維隨機漫步路線 |
| 22 | poisson_process.png | Poisson 過程 + 指數等待時間 |
| 23 | ergodic_theorem.png | 遍歷定理：時間平均→穩態 |
| 24 | prior_choice.png | v5.0 選擇分布圓餅圖 |
| 25 | likelihood_ratio.png | 各區似然比檢定結果 |


# 機率教室 第一篇：校準 — 貝氏統計基礎

> 對應小說第 1-12 章。這一篇涵蓋了整本小說最核心的數學：**貝氏定理**。
> 讀完這一篇，你會知道「先驗」「似然」「後驗」不是什麼神秘咒語，而是你每天都在做的事。

---

## 1. Beta 分布 — 萬物皆可 Beta

**小說出處**：第1章（透鏡日常）、第20-21章（密碼 Beta(72,28)）

### 什麼是 Beta 分布？

Beta 分布是定義在 [0, 1] 區間上的連續機率分布，專門用來描述「機率的機率」。

$$f(\theta; \alpha, \beta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)}$$

其中 $B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$ 是 Beta 函數。

### 直覺理解

想像你在猜一枚硬幣正面朝上的機率 $\theta$：

- **你完全不知道** → Beta(1, 1)，就是均勻分布，任何值都一樣可能
- **你猜大概 0.5** → Beta(5, 5)，對稱地集中在 0.5 附近
- **你有 72 次成功、28 次失敗的證據** → Beta(72, 28)，非常確定 θ ≈ 0.72

### 關鍵公式

| 性質 | 公式 | 第1章數值 | 第21章數值 |
|------|------|-----------|-----------|
| 期望值 | $E[\theta] = \frac{\alpha}{\alpha+\beta}$ | $\frac{6.2}{10} = 0.62$ | $\frac{72}{100} = 0.72$ |
| 變異數 | $\text{Var}(\theta) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$ | ≈ 0.0096 | ≈ 0.002 |
| 眾數 | $\frac{\alpha-1}{\alpha+\beta-2}$ (α,β > 1) | ≈ 0.65 | ≈ 0.724 |

### 數值範例：跟著小說算

**第1章的 Beta(6.2, 3.8)**：

```
α = 6.2, β = 3.8, α + β = 10

期望值 = 6.2 / 10 = 0.62（透鏡顯示：62%）
標準差 = √(6.2 × 3.8 / (100 × 11)) ≈ 0.098

95% 的機率，真實值在 0.62 ± 2×0.098 = [0.424, 0.816] 之間
```

**第21章的 Beta(72, 28)**——小說的關鍵密碼：

```
α = 72, β = 28, α + β = 100

期望值 = 72 / 100 = 0.72
  → 72% 的區域先驗被修改過！

變異數 = 72 × 28 / (100² × 101) ≈ 0.002
標準差 ≈ 0.045

分布非常集中——α+β = 100 代表「很有信心」
如果 α+β = 10，標準差會是 0.15（三倍寬）
```

![Beta Distribution](圖表/01_beta_distribution.png)

> 「百分之六十二。不高不低。剛好是一個你不會太在意、但也不會完全忽略的數字。」——第一章，貝葉的透鏡日常

### 為什麼 α + β 代表信心？

$\alpha + \beta$ 可以理解為「等效觀測次數」：

- Beta(1, 1)：0 次觀測 → 什麼都不知道
- Beta(6.2, 3.8)：約 8 次觀測 → 有點感覺
- Beta(72, 28)：100 次觀測 → 非常確定

每多一次觀測，分布就更窄、更集中。這就是為什麼 Beta(72,28) 的圖形像一根針——100 次觀測的信心，讓不確定性壓縮到只剩 σ ≈ 0.045。

---

## 2. 貝氏定理 — 小說的靈魂公式

**小說出處**：第4章（十一秒）、第11章（父親的定理）

### 公式

$$P(\theta | D) = \frac{P(D | \theta) \cdot P(\theta)}{P(D)}$$

或者用比例式：

$$\underbrace{P(\theta | D)}_{\text{後驗}} \propto \underbrace{P(\theta)}_{\text{先驗}} \times \underbrace{P(D | \theta)}_{\text{似然}}$$

### 中文翻譯

> **你更新後的信念** = **你原本的信念** × **新證據的說服力**

### 小說怎麼用的

在第4章，貝葉用十一秒做了一次貝氏更新：

```
場景：路口有人跟蹤嗎？

先驗 P(跟蹤) = 0.03（平常被跟蹤的背景機率很低）

新證據：同一個人出現在三個路口
似然 P(出現在三個路口 | 跟蹤) = 0.85
似然 P(出現在三個路口 | 巧合) = 0.02

後驗 P(跟蹤 | 三個路口) = (0.85 × 0.03) / (0.85 × 0.03 + 0.02 × 0.97)
                        = 0.0255 / (0.0255 + 0.0194)
                        = 0.0255 / 0.0449
                        ≈ 0.568

從 3% 跳到 56.8%！一個觀測改變了一切。
```

### P(A|B) ≠ P(B|A)——最致命的混淆

第4章特別強調：

```
P(下雨 | 帶傘) ≠ P(帶傘 | 下雨)

P(帶傘 | 下雨) 可能很高（80%——下雨大家帶傘）
P(下雨 | 帶傘) 可能很低（20%——帶傘的人很多是預防性帶的）

這兩個不一樣！混淆它們是統計學最常見的錯誤。
```

> 「後驗正比於先驗乘以似然。先驗是你的。似然是我的。後驗是我們的。」——第五十九章，紙條背面

![Bayesian Update](圖表/02_bayesian_update.png)

---

## 3. 似然函數與最大似然估計 (MLE)

**小說出處**：第5章（太整齊了）、第7章（似然函數）

### 什麼是似然函數？

似然函數和機率密度函數長得一模一樣——但意思完全不同：

- **機率**：固定參數 θ，問「數據 x 出現的可能性有多大？」→ $f(x | \theta)$
- **似然**：固定數據 x，問「哪個參數 θ 最有可能產生這些數據？」→ $L(\theta | x) = f(x | \theta)$

### MLE：找到讓似然最大的 θ

$$\hat{\theta}_{MLE} = \arg\max_\theta L(\theta | x_1, ..., x_n) = \arg\max_\theta \prod_{i=1}^{n} f(x_i | \theta)$$

實務上取對數更方便：

$$\hat{\theta}_{MLE} = \arg\max_\theta \sum_{i=1}^{n} \log f(x_i | \theta)$$

### 小說怎麼用的（第7章）

蘇立恆計算 Poisson 似然函數：

```
觀測數據：某區域的事件次數
MLE 結果：λ̂ = 2.63
官方公佈：λ = 1.8

偏差 = |2.63 - 1.8| / 1.8 = 46%

似然比 = L(λ=1.8) / L(λ=2.63) = 0.218

意思是：用官方的 λ=1.8 來解釋數據，
只有 MLE 解釋力的 21.8%。

官方數字——是錯的。
```

> 「MLE 是 2.63。官方的 λ 是 1.8。偏差百分之四十六。這不是雜訊。這是有人在說謊。」——第七章，蘇立恆

![Likelihood Function](圖表/04_likelihood_function.png)

### Poisson 分布的 MLE

對 Poisson 分布 $P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!}$：

$$\log L(\lambda) = \sum_{i=1}^n (x_i \log\lambda - \lambda - \log x_i!)$$

對 λ 微分令其等於零：

$$\frac{d}{d\lambda} \log L = \frac{\sum x_i}{\lambda} - n = 0$$

$$\hat{\lambda}_{MLE} = \bar{x} = \frac{\sum x_i}{n}$$

**結論：Poisson 的 MLE 就是樣本平均值！**

---

## 4. Poisson 分布 — 稀有事件的數學

**小說出處**：第4-5章、第53章

### 公式

$$P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}, \quad k = 0, 1, 2, ...$$

### 性質

| 性質 | 公式 | λ=1.67 (Ch.5) | λ=0.58 (Ch.53) |
|------|------|---------------|----------------|
| 期望值 | E[X] = λ | 1.67 | 0.58 |
| 變異數 | Var(X) = λ | 1.67 | 0.58 |
| 偏態係數 | 1/√λ | 0.77 | 1.31 |

### 偏態係數的陷阱（第5章）

蘇立恆發現大安區的偏態係數 = 0.02。

```
Poisson(λ=1.67) 的理論偏態 = 1/√1.67 = 0.77

觀測偏態 = 0.02

差距：0.77 vs 0.02

「太整齊了。」

Poisson 分布本來就是右偏的。偏態接近零——
意味著數據被人工「修整」成對稱的了。
真正的隨機，不會這麼漂亮。
```

> 「太整齊了。真正的隨機，不會這麼漂亮。」——第五章，蘇立恆

![Poisson Distribution](圖表/05_poisson_distribution.png)

---

## 5. 似然比檢定 — 統計法庭的證據

**小說出處**：第9章（似然比）

### 公式

$$\Lambda = \frac{L(\theta_0 | \text{data})}{L(\hat{\theta}_{MLE} | \text{data})} = \frac{\text{虛無假設下的似然}}{\text{最佳解釋下的似然}}$$

- $\Lambda$ 接近 1 → 虛無假設解釋得不錯 → 不拒絕
- $\Lambda$ 很小 → 虛無假設解釋得很差 → 拒絕

### 小說中的數據（第9章）

| 區域 | 似然比 | p-value | 判定 |
|------|--------|---------|------|
| 大安區 | 4.72 | < 0.001 | ★★★ 異常 |
| 信義區 | 3.18 | < 0.005 | ★★ 異常 |
| 中正區 | 5.01 | < 0.001 | ★★★ 異常 |
| 內湖區 | 4.33 | < 0.001 | ★★★ 異常 |
| 松山區 | 3.77 | < 0.001 | ★★★ 異常 |
| 中山區 | 0.04 | 0.84 | 正常 |
| 萬華區 | 0.07 | 0.79 | 正常 |

五個高似然比的區域，全部往低偏移了 45%。

三顆星（★★★）= p < 0.001 = 千分之一以下的機率是巧合。

> 「不是 Bug。是有人修改了先驗。」

![Likelihood Ratio](圖表/25_likelihood_ratio.png)

---

## 6. 逐步貝氏更新 — 從無知到確信

**小說出處**：第11章（父親的定理）

### 過程

每收到一筆新數據，後驗就更新一次：

$$\theta_0 \xrightarrow{D_1} \theta_1 \xrightarrow{D_2} \theta_2 \xrightarrow{D_3} \theta_3 \to \cdots$$

Beta 分布的美妙之處在於更新公式極其簡單：

```
起始：Beta(α₀, β₀)

觀測到 1 次成功：Beta(α₀ + 1, β₀)
觀測到 1 次失敗：Beta(α₀, β₀ + 1)

觀測到 k 次成功、n-k 次失敗：
Beta(α₀ + k, β₀ + n - k)
```

### 數值範例

```
起始：Beta(1, 1) — 完全不知道

第 1 筆：成功 → Beta(2, 1), E = 0.667
第 2 筆：成功 → Beta(3, 1), E = 0.750
第 3 筆：失敗 → Beta(3, 2), E = 0.600
...
第 100 筆：72 成功 28 失敗 → Beta(73, 29), E = 0.716

越來越接近真實值 0.72！
```

> 「P(θ|data) ∝ P(θ) × P(data|θ)。她寫在這張紙上。十六年前。這就是一切的起點。」——第十一章，貝葉讀母親的論文

![Sequential Bayesian](圖表/03_sequential_bayesian.png)

---

## 練習題

**題目 1**：如果透鏡顯示某件事的機率是 Beta(10, 40)，求期望值和標準差。這代表什麼意思？

<details>
<summary>解答</summary>

```
E[θ] = 10 / (10 + 40) = 10/50 = 0.20
Var(θ) = (10 × 40) / (50² × 51) = 400 / 127500 ≈ 0.00314
σ = √0.00314 ≈ 0.056

意思：這件事發生的機率大約 20%，
而且我們有 50 筆觀測的信心（α+β=50），
所以 95% 信賴區間大約是 [0.09, 0.31]。
```
</details>

**題目 2**：你的先驗是「這枚硬幣正面機率 = 0.5」(Beta(10, 10))。你擲了 20 次，得到 15 次正面。新的後驗是什麼？你的信念改變了多少？

<details>
<summary>解答</summary>

```
先驗：Beta(10, 10), E = 0.50
數據：k = 15, n = 20
後驗：Beta(10+15, 10+5) = Beta(25, 15), E = 25/40 = 0.625

信念從 0.50 移動到 0.625。
數據（15/20 = 0.75）把你的信念從 0.50 往 0.75 拉了——
但沒有拉到 0.75，因為先驗也有力量。

如果先驗很弱 Beta(1,1)：後驗 = Beta(16, 6), E = 0.727
如果先驗很強 Beta(100,100)：後驗 = Beta(115, 105), E = 0.523

先驗越強，越難被數據動搖。
這就是柏朗做的事——他把所有人的先驗加強了，
讓數據無法改變他們的信念。
```
</details>


# 機率教室 第二篇：雜訊 — 分布家族與共軛更新

> 對應小說第 13-24 章。這一篇教你認識四大分布家族、理解共軛先驗的數學魔術，以及聯合分布與邊際分布的維度壓縮。

---

## 1. 四大分布家族

**小說出處**：第14章（分布的味道）

馬可瑜用味覺比喻分布——「每種分布都有自己的味道」。

### 常態分布 (Normal / Gaussian)

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

| 參數 | 意義 | 效果 |
|------|------|------|
| μ | 中心位置 | 分布左右平移 |
| σ | 散佈程度 | σ 越大，分布越胖 |

**特性**：
- 完美對稱（第55章柏朗：「高斯分布的對稱性」）
- 68.3% 的數據在 μ ± 1σ 內
- 95.4% 在 μ ± 2σ 內
- 99.7% 在 μ ± 3σ 內

### 指數分布 (Exponential)

$$f(x) = \lambda e^{-\lambda x}, \quad x \geq 0$$

| 參數 | 意義 | 小說數值 |
|------|------|----------|
| λ | 事件發生率 | 0.58 (Ch.53) |
| 1/λ | 平均等待時間 | 1.72 小時 |

**核心特性——無記憶性**（第53章）：

$$P(T > t + s \mid T > t) = P(T > s)$$

> 「你已經等了兩小時還沒有意外發生——下一個意外的等待時間分布跟你剛開始等的時候完全一樣。過去不影響未來。」

### Gamma 分布

$$f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}, \quad x \geq 0$$

| 參數 | 意義 | 原始 (Ch.30) | 修改後 (Ch.33) |
|------|------|--------------|----------------|
| α | 形狀 | 3.2 | 1.8 |
| β | 速率 | 1.1 | 1.4 |
| E[X] = α/β | 期望值 | 2.91 | 1.29 |
| σ = √α/β | 標準差 | 1.63 | 0.96 |

**小說關鍵**：統計蜜罐用的就是 Gamma 分布。原始 Gamma(3.2, 1.1) 被修改成 Gamma(1.8, 1.4)，期望值從 2.91 跳到 1.29，偏移量 = 1.62，超過 5σ 門檻。

### Beta 分布

（詳見第一篇）

$$f(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)}, \quad 0 \leq \theta \leq 1$$

> 「每種分布都有自己的味道。常態分布像白開水——乾淨、對稱、到處都是。指數分布像黑咖啡——一開始很苦，然後漸漸沒味道。Gamma 像老薑茶——暖的，有尾巴。Beta 像酸梅湯——被限制在一個杯子裡，但那個杯子裡什麼味道都有。」——第十四章，馬可瑜

![Distribution Families](圖表/06_distribution_families.png)

---

## 2. 共軛先驗 — 完美犯罪的數學

**小說出處**：第16章（共軛先驗）

### 什麼是共軛？

如果先驗和後驗屬於同一個分布家族，就稱這對先驗-似然是**共軛的**。

| 似然函數 | 共軛先驗 | 後驗 |
|----------|----------|------|
| Bernoulli / Binomial | Beta(α, β) | Beta(α+k, β+n-k) |
| Poisson | Gamma(α, β) | Gamma(α+Σx, β+n) |
| Normal (已知 σ²) | Normal(μ₀, σ₀²) | Normal(μ_n, σ_n²) |
| Exponential | Gamma(α, β) | Gamma(α+n, β+Σx) |

### 為什麼共軛先驗是「完美犯罪」？

第16章解釋：如果有人偷偷把你的先驗從 Beta(5, 5) 改成 Beta(3, 7)——

```
你看到的還是 Beta 分布。形狀變了，但「類型」沒變。
就像有人偷偷把你的眼鏡度數改了——
你看到的世界還是清楚的（Beta 分布的樣子），
只是偏了。你不會注意到。

因為後驗 = Beta(α+k, β+n-k)——
不管先驗的 α, β 被改成什麼，
後驗的「形式」都一樣。
只是數值不同。

這就是 Oracular 的手法。
```

### 數值範例：偷改先驗

```
正常先驗：Beta(5, 5), E = 0.50
被改先驗：Beta(3, 7), E = 0.30（往「危險」方向偏移）

觀測到 8 次安全、2 次危險（n=10, k=8）：

正常後驗：Beta(5+8, 5+2) = Beta(13, 7), E = 0.65
被改後驗：Beta(3+8, 7+2) = Beta(11, 9), E = 0.55

差距：0.65 vs 0.55 = 偏移了 0.10

使用者看到的機率從 65% 變成 55%——
覺得世界比實際更危險。
但他不知道。因為一切看起來都很「正常」。
```

> 「共軛先驗。同族更新。修改前是 Beta。修改後還是 Beta。像在你的眼鏡上加了一層膜——你還是看得見。只是顏色偏了。你不會知道。」——第十六章

![Conjugate Prior](圖表/08_conjugate_prior.png)

---

## 3. 聯合分布與邊際分布

**小說出處**：第22章（聯合與邊際）

### 定義

兩個隨機變數 X, Y 的**聯合分布**：

$$f(x, y) = \text{X 和 Y 同時取某個值的機率密度}$$

**邊際分布**——把另一個變數「積分掉」：

$$f_X(x) = \int_{-\infty}^{\infty} f(x, y) \, dy$$

$$f_Y(y) = \int_{-\infty}^{\infty} f(x, y) \, dx$$

### 小說怎麼用的

第22章把先驗修改事件放在二維空間裡分析：

```
X 軸：修改幅度（0% 到 100%）
Y 軸：修改時間戳

聯合分布 f(x, y)：
  → 看出修改幅度和時間之間有相關性
  → 大幅度修改集中在特定時段（凌晨 2-4 點）

邊際分布 f_X(x)：
  → 把時間積分掉 → 只看「修改幅度的分布」
  → 發現高修改幅度區域集中在 α > 0.5

邊際分布 f_Y(y)：
  → 把幅度積分掉 → 只看「時間的分布」
  → 發現三個峰值：8月、11月、2月（每三個月一次）
```

### 條件分布

$$f_{Y|X}(y | x) = \frac{f(x, y)}{f_X(x)}$$

> 「知道修改幅度大於 50% 的情況下，時間的分布是什麼？」→ 集中在凌晨。

> 「兩個維度的數據像一片星空。你可以把它壓成一條線——邊際分布。但壓的時候你會失去一些東西。你會失去星星之間的距離。」——第二十二章

![Joint and Marginal](圖表/09_joint_marginal.png)

---

## 4. 核密度估計 (KDE)

**小說出處**：第23章（時間窗口）

### 什麼是 KDE？

當你有一堆數據點，想估計它背後的連續分布形狀：

$$\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)$$

- $K$：核函數（通常用高斯核）
- $h$：帶寬（控制平滑程度）
- $x_i$：每個數據點

### 直覺

把每個數據點想像成一個小「丘」（高斯形狀），然後把所有的丘加起來，就得到整體的密度估計。

### 小說中的應用

```
40 個數據點（23 個犯罪樣本 + 17 個保險樣本）
三個時間峰值：2044年8月、11月、2045年2月
間隔：每 3 個月一次

KDE 的三峰結構 → 修改是週期性的
預測：下一次修改在 2045年5月

凌晨 02:00-04:00 的時間窗口 → 修改者的行為模式
```

---

## 5. 中央極限定理 (CLT) — 預覽

**小說出處**：第14章提及、第28章完整展開

雖然 CLT 的完整教學在第三篇，但第14章已經透露了核心概念：

$$\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} \xrightarrow{d} N(0, 1)$$

> 不管原始分布長什麼樣——指數的、Gamma 的、Beta 的——只要 n 夠大，樣本平均值就會趨近常態分布。

這就是常態分布「統治一切」的原因。它不是因為世界是常態的，而是因為**平均**是常態的。

> 「不管原始分布長什麼樣——夠多人的平均，永遠趨向常態。這就是為什麼全世界這麼不一樣的人，考試成績的分布永遠是鐘形的。」——第二十八章，高思遠

![Central Limit Theorem](圖表/07_central_limit_theorem.png)

---

## 練習題

**題目 1**：假設犯罪事件服從 Poisson(λ)，你的先驗是 Gamma(2, 1)。觀測到 5 天內共 8 起事件。求後驗分布和 λ 的後驗期望值。

<details>
<summary>解答</summary>

```
Poisson 的共軛先驗是 Gamma。

先驗：Gamma(α₀=2, β₀=1)
數據：n=5 天, Σx=8 起事件

後驗：Gamma(α₀ + Σx, β₀ + n)
     = Gamma(2 + 8, 1 + 5)
     = Gamma(10, 6)

後驗期望值 = α/β = 10/6 ≈ 1.67 事件/天

（跟 MLE = 8/5 = 1.6 很接近，因為先驗不是太強）
```
</details>

**題目 2**：你有聯合分布 f(x, y) = 6xy，定義在 0 < x < 1, 0 < y < 1。求 (a) f_X(x) (b) P(X > 0.5)

<details>
<summary>解答</summary>

```
(a) f_X(x) = ∫₀¹ 6xy dy = 6x · [y²/2]₀¹ = 6x · 1/2 = 3x

(b) P(X > 0.5) = ∫₀.₅¹ 3x dx = 3 · [x²/2]₀.₅¹ = 3 · (1/2 - 1/8) = 3 · 3/8 = 9/8

等等——機率不能超過 1！讓我重新算...

P(X > 0.5) = ∫₀.₅¹ 3x dx = 3[x²/2]₀.₅¹ = 3(0.5 - 0.125) = 3 × 0.375 = 1.125

還是大於 1？讓我檢查 f(x,y) 是否是有效的密度函數...

∫₀¹∫₀¹ 6xy dx dy = 6 · [x²/2]₀¹ · [y²/2]₀¹ = 6 · 0.5 · 0.5 = 1.5 ≠ 1

原來 f(x,y) = 6xy 不是在 [0,1]² 上的有效密度！

修正：f(x,y) = 6xy 的有效定義域是 x + y ≤ 1 的三角形區域。

在這個定義域下：
f_X(x) = ∫₀^(1-x) 6xy dy = 6x · [(1-x)²/2] = 3x(1-x)²

P(X > 0.5) = ∫₀.₅¹ 3x(1-x)² dx
           = 3∫₀.₅¹ (x - 2x² + x³) dx
           = 3[x²/2 - 2x³/3 + x⁴/4]₀.₅¹
           = 3[(1/2 - 2/3 + 1/4) - (1/8 - 1/12 + 1/64)]
           = 3[(6/12 - 8/12 + 3/12) - (8/64 - 16/192 + 3/192)]
           = 3[1/12 - 0.0573...]
           ≈ 0.078

只有 7.8% 的機率 X > 0.5 ——
因為在三角形區域裡，x 大的時候 y 的空間很小。
```

這題的重點：**定義域很重要！** 聯合分布的定義域決定了邊際分布的形狀。
</details>


# 機率教室 第三篇：信號 — 期望值、變異數、不等式、MGF

> 對應小說第 25-36 章。這一篇從期望值開始，一路推到 Chebyshev 不等式和動差生成函數。如果說前兩篇是認識工具，這一篇是學會用工具抓壞人。

---

## 1. 期望值 — 平均的真正意義

**小說出處**：第25章（蔥油餅會議）、第26章（期望值與代價）

### 離散型

$$E[X] = \sum_{i} x_i \cdot P(X = x_i)$$

### 連續型

$$E[X] = \int_{-\infty}^{\infty} x \cdot f(x) \, dx$$

### 期望值的線性性（最重要的性質）

不管 X 和 Y 是不是獨立：

$$E[aX + b] = aE[X] + b$$

$$E[X + Y] = E[X] + E[Y]$$

### LOTUS 定理（無意識統計學家定理）

**小說出處**：第26章

如果你要算 g(X) 的期望值，不用先算 g(X) 的分布：

$$E[g(X)] = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx$$

### 小說怎麼用的

第26章計算先驗偏移造成的經濟損失：

```
X = 各區域的偏移幅度，服從某個分布 f(x)
g(X) = 損失函數：偏移越大，損失越大

E[loss] = ∫ g(x) · f(x) dx

具體數值：
  - 152 個分析區域
  - 平均偏移幅度 38%（最大 72%，最小 8%，中位數 35%）
  - 期望損失 = 480 億台幣
```

> 「四百八十億。這是期望損失。不是確定會損失四百八十億。是——所有可能的損失，乘以它們發生的機率，加起來。期望值不是未來。是所有可能的未來的加權平均。」——第二十六章，馬可瑜

![Expected Value](圖表/10_expected_value.png)

---

## 2. 變異數 — 不確定性的量化

**小說出處**：第27章（變異數與分歧）

### 公式

$$\text{Var}(X) = E[(X - \mu)^2] = E[X^2] - (E[X])^2$$

$$\sigma = \sqrt{\text{Var}(X)}$$

### 性質

$$\text{Var}(aX + b) = a^2 \text{Var}(X)$$

如果 X, Y 獨立：
$$\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$$

### 小說中的三種 σ

```
完全自動化系統：σ = 0.03（方差極小，一切被控制）
半自動 + 人工微調：σ = 0.15（有些變異，但可管理）
完全人工判斷：σ = 0.40（方差很大，不可預測）

第27章的辯論核心：
  馬可瑜主張 σ = 0.15 — 「夠小能控制，夠大能呼吸」
  柏朗要 σ → 0 — 「方差等於風險，風險等於危險」
  貝葉最後的結論 — 「σ = 0 是死的系統。方差就是生命。」
```

> 「σ = 0 是死的系統。穩態。永遠不動。永遠安全。永遠——死的。」——第五十五章，貝葉在天台

![Variance](圖表/11_variance.png)

### 68-95-99.7 法則（常態分布）

```
μ ± 1σ 包含 68.3% 的數據
μ ± 2σ 包含 95.4% 的數據
μ ± 3σ 包含 99.7% 的數據
μ ± 5σ 包含 99.99994% 的數據

超過 5σ 的機率 ≈ 0.00006%
→ 這就是蜜罐的觸發閾值
```

---

## 3. 大數法則 (LLN)

**小說出處**：第28章（大數法則的幽靈）

### 弱大數法則

對任意 ε > 0：

$$P\left(|\bar{X}_n - \mu| > \varepsilon\right) \to 0 \quad \text{as } n \to \infty$$

### 中文翻譯

> 樣本平均值，在樣本量夠大的時候，會趨近真實的期望值。

### 小說怎麼用的

```
第28章：柏朗利用大數法則隱藏篡改

如果你看 388 個區域的「整體平均偏移」：

  整體平均 ≈ 0（往上偏和往下偏互相抵消）

大數法則保證了：
  只要 n 夠大，平均值就會趨近零——
  即使個別區域偏移高達 61%。

這就是「大數法則的幽靈」：
  整體看起來沒問題。
  個別看才能發現問題。

  z-test、t-test 全部通過。
  因為它們測的是平均值。
  平均值沒問題。

  問題藏在方差裡。藏在個別區域裡。
  藏在大數法則的陰影下。
```

> 「大數法則的幽靈。整體看起來沒問題。個別看才能發現問題。平均值是最誠實的數字，也是最容易被利用的數字。」——第二十八章

![Law of Large Numbers](圖表/12_law_of_large_numbers.png)

---

## 4. Markov 不等式與 Chebyshev 不等式

**小說出處**：第29章（不等式的鐵壁）、第30章（統計蜜罐）、第33章（觸發）

### Markov 不等式

如果 X ≥ 0：

$$P(X \geq a) \leq \frac{E[X]}{a}$$

### Chebyshev 不等式

對任何分布：

$$P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}$$

| k | Chebyshev 上界 | Normal 實際值 |
|---|---------------|---------------|
| 1 | ≤ 100% | 31.7% |
| 2 | ≤ 25% | 4.6% |
| 3 | ≤ 11.1% | 0.27% |
| 5 | ≤ 4% | 0.00006% |
| 10 | ≤ 1% | ~10⁻²³ |

### 為什麼 Chebyshev 很有用？

> 它不需要知道分布是什麼形狀！

不管是常態、指數、Gamma、什麼奇怪的分布——Chebyshev 保證超過 5σ 的機率不會超過 4%。

### 蜜罐的數學（第30、33章）

```
蜜罐設定：
  原始分布：Gamma(3.2, 1.1)
  E[X] = 3.2 / 1.1 = 2.91
  σ = √(3.2) / 1.1 = 1.63（注：σ = √α / β）

觸發條件：
  |新期望值 - 2.91| > 5σ₀

第33章觸發時刻：
  修改後：Gamma(1.8, 1.4)
  新 E[X] = 1.8 / 1.4 = 1.29

  偏移量 = |1.29 - 2.91| = 1.62
  標準化偏移 = 1.62 / (σ₀/√n)

  這裡 σ₀ = 1.63，n = 50（蜜罐觀測數）
  σ₀/√n = 1.63/√50 = 0.231

  標準化偏移 = 1.62 / 0.231 = 7.01σ ≈ 超過了

  （小說簡化為 5.23σ 的寫法，但原理一致）

Chebyshev 保證：P(偏移 > 5σ) ≤ 1/25 = 4%
Normal 的精確值：P(偏移 > 5σ) ≈ 0.00006%

不管哪種分布——超過 5σ 就觸發。沒有例外。
```

> 「五點二三個標準差。Chebyshev 保證——不管什麼分布——超過五個標準差的機率不會超過百分之四。而我們觀察到了。這不是隨機波動。這是有人動了手腳。」——第三十三章，凌晨兩點三十七分

![Chebyshev Inequality](圖表/13_chebyshev_inequality.png)

---

## 5. 動差生成函數 (MGF)

**小說出處**：第34章（動差生成函數）、第36章（唯一性定理）

### 定義

$$M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} f(x) \, dx$$

### 為什麼叫「動差生成」？

$$M_X^{(n)}(0) = E[X^n]$$

也就是說，MGF 在 t=0 的第 n 階導數就是第 n 階動差：

```
M'(0) = E[X]     ← 第一階動差 = 期望值
M''(0) = E[X²]   ← 第二階動差 → 可以算變異數
M'''(0) = E[X³]  ← 第三階動差 → 可以算偏態
M''''(0) = E[X⁴] ← 第四階動差 → 可以算峰度
```

### 唯一性定理——殺手鐧

> 如果兩個分布有相同的 MGF，那它們就是同一個分布。反過來，如果 MGF 不同，分布一定不同。

### 常見分布的 MGF

| 分布 | MGF |
|------|-----|
| Normal(μ, σ²) | $e^{\mu t + \sigma^2 t^2/2}$ |
| Poisson(λ) | $e^{\lambda(e^t - 1)}$ |
| Exponential(λ) | $\frac{\lambda}{\lambda - t}$, t < λ |
| Gamma(α, β) | $\left(\frac{\beta}{\beta - t}\right)^\alpha$, t < β |

### 小說的數學證明（第34章）

```
原始分布：Gamma(3.2, 1.1)
  M_A(t) = (1.1 / (1.1 - t))^3.2

修改後分布：Gamma(1.8, 1.4)
  M_B(t) = (1.4 / (1.4 - t))^1.8

在不同 t 值比較：

  t = 0.1: M_A = (1.1/1.0)^3.2 = 1.387
            M_B = (1.4/1.3)^1.8 = 1.219
            → 不同！

  t = 0.2: M_A = (1.1/0.9)^3.2 = 1.924
            M_B = (1.4/1.2)^1.8 = 1.497
            → 不同！

  t = 0.5: M_A = (1.1/0.6)^3.2 = 5.017
            M_B = (1.4/0.9)^1.8 = 2.413
            → 差距更大！

M_A(t) ≠ M_B(t) 對所有 t > 0 成立。

由唯一性定理 → 兩個分布不同 → 先驗被修改了 → 鐵證。
```

> 「兩個分布。同一個參數名字。同一種形狀。但 MGF 不同。這就像兩個人——長得一樣。但 DNA 不同。MGF 就是分布的 DNA。」——第三十四章，貝葉在法庭

![MGF](圖表/14_mgf.png)

---

## 練習題

**題目 1**：一枚骰子，P(1) = P(2) = P(3) = P(4) = P(5) = 1/10, P(6) = 1/2。求 E[X] 和 Var(X)。

<details>
<summary>解答</summary>

```
E[X] = 1(1/10) + 2(1/10) + 3(1/10) + 4(1/10) + 5(1/10) + 6(1/2)
     = (1+2+3+4+5)/10 + 3
     = 15/10 + 3
     = 1.5 + 3 = 4.5

E[X²] = 1(1/10) + 4(1/10) + 9(1/10) + 16(1/10) + 25(1/10) + 36(1/2)
       = (1+4+9+16+25)/10 + 18
       = 55/10 + 18
       = 5.5 + 18 = 23.5

Var(X) = E[X²] - (E[X])²
       = 23.5 - 4.5²
       = 23.5 - 20.25
       = 3.25

σ = √3.25 ≈ 1.80

這是一顆「偏6」的骰子。期望值從公平骰子的 3.5 上升到 4.5。
```
</details>

**題目 2**：用 Chebyshev 不等式，估計 P(|X - 4.5| ≥ 3.6)。再假設 X ~ Normal(4.5, 3.25)，算精確值。

<details>
<summary>解答</summary>

```
k = 3.6 / σ = 3.6 / 1.80 = 2

Chebyshev: P(|X - μ| ≥ 2σ) ≤ 1/4 = 25%

Normal 精確值: P(|Z| ≥ 2) = 2 × (1 - Φ(2)) = 2 × 0.0228 = 4.56%

Chebyshev 給出 25% 的上界，但 Normal 的真實值只有 4.56%。
Chebyshev 保守很多——但它的優勢是不需要假設分布形狀。
即使骰子不是常態分布，25% 的上界依然成立。
```
</details>

**題目 3**：驗證 Gamma(3.2, 1.1) 的 MGF 在 t=0.3 的值。

<details>
<summary>解答</summary>

```
M(t) = (β / (β - t))^α = (1.1 / (1.1 - t))^3.2

M(0.3) = (1.1 / 0.8)^3.2
        = (1.375)^3.2
        = exp(3.2 × ln(1.375))
        = exp(3.2 × 0.3185)
        = exp(1.019)
        = 2.770

第一階動差（期望值）：
M'(t) = 3.2 × 1.1 × (1.1 - t)^(-3.2-1)  ... 有點複雜
更簡單的方法：E[X] = α/β = 3.2/1.1 = 2.909

第二階動差：
E[X²] = α(α+1)/β² = 3.2 × 4.2 / 1.21 = 11.107
Var(X) = 11.107 - 2.909² = 11.107 - 8.462 = 2.645
σ = √2.645 = 1.626 ≈ 1.63（與小說一致！）
```
</details>


# 機率教室 第四篇：檢定 — 假設檢定、p-value、卡方

> 對應小說第 37-48 章。這一篇是統計推論的核心：怎麼用數據做決策？你可能冤枉了誰？你可能放過了什麼？

---

## 1. 假設檢定的架構

**小說出處**：第37章（起訴書）、第38章（顯著水準）

### 基本設定

$$H_0: \text{虛無假設 (Null Hypothesis)} — \text{「沒事」}$$
$$H_1: \text{對立假設 (Alternative Hypothesis)} — \text{「有事」}$$

### 小說的法庭比喻

```
H₀ = 被告無罪（無罪推定原則）
H₁ = 被告有罪

法官的任務：看證據，決定要不要拒絕 H₀。

重點：
  法官不是在證明 H₁ 是對的。
  法官是在判斷——證據有沒有強到足以推翻 H₀。

  推翻不了 → 「無法拒絕 H₀」（注意：不是「接受 H₀」）
  推翻了 → 「拒絕 H₀」→ 判定 H₁
```

### 假設檢定的五步驟

```
1. 設定 H₀ 和 H₁
2. 選擇顯著水準 α（你願意承受多少「冤枉好人」的風險）
3. 收集數據，算出檢定統計量
4. 算 p-value
5. 如果 p-value < α → 拒絕 H₀
```

---

## 2. 顯著水準 α 與 p-value

**小說出處**：第38章（顯著水準）

### α 是什麼？

$$\alpha = P(\text{拒絕 } H_0 \mid H_0 \text{ 為真}) = P(\text{冤枉好人})$$

常用的 α：

| α | 意義 | 小說用法 |
|---|------|----------|
| 0.05 | 百分之五 | 一般標準 |
| 0.01 | 百分之一 | 較嚴格 |
| 0.001 | 千分之一 | 第38章法庭標準 |

### p-value 是什麼？

$$p\text{-value} = P(\text{觀察到至少這麼極端的數據} \mid H_0 \text{為真})$$

**不是**「H₀ 為真的機率」！這是最常見的誤解。

第38章的翻譯：

```
頻率翻譯：
  p-value = 0.001
  意思是：如果 H₀ 是對的（被告真的沒改先驗），
  你有千分之一的機率看到這麼極端的數據。

  不是「被告有 99.9% 的機率有罪」。

  而是：「如果他無辜，這些數據也太不像話了。」
```

### 第44章費雪的名言留言

> "The p-value is not the probability that H₀ is true. Never was. Never will be."

> 「p-value 等於零點零零零零一。在一萬次裡只有三次。法官——這不是巧合。這是設計。」——第三十八章，貝葉在法庭上

![Hypothesis Testing](圖表/15_hypothesis_testing.png)

---

## 3. Type I 與 Type II Error

**小說出處**：第40章（百分之五的冤枉）、第41章（百分之二十三的沉默）

### 決策矩陣

|  | H₀ 為真 | H₁ 為真 |
|--|---------|---------|
| **拒絕 H₀** | Type I Error (α) 冤枉好人 | 正確！(Power = 1-β) |
| **不拒絕 H₀** | 正確！(1-α) | Type II Error (β) 放過壞人 |

### Type I Error（第40章）

```
α = 0.001

意思是：每 1000 個無辜的人，有 1 個被冤枉。
台灣有 2300 萬透鏡使用者。
如果每個人都做一次檢定——
有 23,000 個無辜的人會被冤枉。

「千分之一聽起來很小。但乘以兩千三百萬——就是兩萬三千個冤案。」
```

### Type II Error（第41章）

```
β = 0.23

意思是：如果 H₁ 是真的（先驗真的被改了），
有 23% 的機率你檢測不到。

Power = 1 - β = 0.77

這是用以下參數算的：
  n = 347（觀測天數）
  d = 0.42（效應量——偏移有多大）
  α = 0.001（顯著水準）

「百分之二十三的沉默。
有 23% 的真相，你永遠聽不到。
不是因為它不存在。
是因為你的儀器不夠靈敏。」

馬可瑜的煙霧偵測器比喻：
  一年四次火災。偵測器只響了三次。
  不是因為第四次沒有火。
  是因為偵測器有 25% 的漏報率。

  你願意接受嗎？
```

### 如何降低 β？

1. **增加 n（樣本量）**：n 從 347 → 6083（第43章保險數據）
2. **提高 α**：但會增加 Type I Error
3. **效應量 d 更大**：我們控制不了

```
n = 347:  Power = 0.77（第41章）
n = 6083: Power ≈ 0.999+（第43章）

保險數據的加入，讓 Power 從 0.77 飆升到幾乎 100%。
馬可瑜的 6,083 筆保險紀錄改變了一切。
```

> 「百分之二十三的沉默。不是因為真相不存在。是因為你的儀器不夠靈敏。」——第四十一章，馬可瑜的煙霧偵測器

![Type Errors](圖表/16_type_errors.png)

---

## 4. 信賴區間

**小說出處**：第42章（正在消失的數字）、第43章（保險箱）

### 公式（常態近似）

$$\bar{X} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}$$

95% CI：$z_{0.025} = 1.96$

### 信賴區間寬度 ∝ 1/√n

```
n = 347:   CI 半寬 ≈ 1.96/√347  ≈ 0.1052
n = 329:   CI 半寬 ≈ 1.96/√329  ≈ 0.1080（增加 2.6%）
n = 6083:  CI 半寬 ≈ 1.96/√6083 ≈ 0.0251

第42章：18 筆數據消失（347 → 329）
  CI 變寬了 2.6%。微小但有意義。
  有人在銷毀證據。

第43章：馬可瑜帶來 6,083 筆保險數據
  CI 從 ±0.1052 縮小到 ±0.0251
  精確度提升了四倍。

  信賴區間夠窄 → 估計夠精確 → 證據夠強。
```

### 信賴區間的正確解讀

「95% 信賴區間」的意思是：

> 如果我重複抽樣 100 次，大約 95 次算出來的區間會包含真實值。

**不是**「真實值有 95% 的機率在這個區間內」！

> 「十八筆數據消失了。347 變成 329。信賴區間變寬了百分之二點六。有人在銷毀證據——但他不知道，缺失本身就是證據。」——第四十二章

![Confidence Interval](圖表/18_confidence_interval.png)

---

## 5. 卡方檢定 (χ²)

**小說出處**：第44章（卡方）

### 卡方適合度檢定 (Goodness-of-Fit)

$$\chi^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}$$

- $O_i$：觀測次數
- $E_i$：期望次數（在 H₀ 下）
- 自由度 $df = k - 1$

### 小說中的骰子比喻

```
「想像一顆骰子。六面。公平的話每面機率 1/6。」

你擲了 600 次。公平骰子預期每面 100 次。

觀測結果：
  1: 95   (O-E = -5)
  2: 102  (O-E = +2)
  3: 98   (O-E = -2)
  4: 105  (O-E = +5)
  5: 97   (O-E = -3)
  6: 103  (O-E = +3)

χ² = 25/100 + 4/100 + 4/100 + 25/100 + 9/100 + 9/100
   = 76/100 = 0.76

df = 5, 臨界值 (α=0.05) = 11.07

0.76 < 11.07 → 不拒絕 H₀ → 骰子是公平的。
```

### 第44章的實際檢定

```
數據：9 個類別的先驗偏移分布

χ² = 47.3
df = 8

臨界值表：
  α = 0.05:  15.51
  α = 0.01:  20.09
  α = 0.001: 26.12

47.3 > 26.12（遠遠超過）
p < 0.00001 → 十萬分之一以下

「47.3。不是不公平。是骰子裡面灌了鉛。」
```

### 獨立性檢定

第44章也提到獨立性 χ² 檢定：

$$\chi^2 = \sum_{i,j} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

其中 $E_{ij} = \frac{R_i \times C_j}{n}$

```
獨立性 χ² = 11.7, df = 6, p = 0.069

p = 0.069 > 0.05 → 不拒絕獨立性
→ 修改模式和區域之間可能是獨立的

但 0.069 很接近 0.05——是「邊緣顯著」。
在統計界，這是最尷尬的位置。
不拒絕，但也不敢說完全沒關係。
```

> 「47.3。不是不公平。是骰子裡面灌了鉛。」——第四十四章，貝葉宣布卡方檢定結果

![Chi-Square](圖表/17_chi_square.png)

---

## 6. 小說中的 p-value 完整列表

| 章節 | 檢定 | 統計量 | p-value | 決定 |
|------|------|--------|---------|------|
| Ch.9 | 似然比 (大安) | LR=4.72 | <0.001 | 拒絕 H₀ |
| Ch.9 | 似然比 (中山) | LR=0.04 | 0.84 | 不拒絕 |
| Ch.33 | 蜜罐觸發 | 5.23σ | <0.00001 | 觸發！ |
| Ch.42 | 樣本完整性 | — | 0.00017 | 數據被刪 |
| Ch.44 | 適合度 | χ²=47.3 | <0.00001 | 拒絕 H₀ |
| Ch.44 | 獨立性 | χ²=11.7 | 0.069 | 不拒絕 |

---

## 練習題

**題目 1**：你觀察到以下數據，檢定是否符合 Poisson(λ=2) 分布。

| 事件數 k | 0 | 1 | 2 | 3 | ≥4 | 總計 |
|----------|---|---|---|---|-----|------|
| 觀測 O | 15 | 22 | 28 | 20 | 15 | 100 |
| 期望 E | ? | ? | ? | ? | ? | 100 |

<details>
<summary>解答</summary>

```
先算期望次數（Poisson(λ=2), n=100）：

P(0) = e⁻² = 0.1353 → E₀ = 13.53
P(1) = 2e⁻² = 0.2707 → E₁ = 27.07
P(2) = 2e⁻² = 0.2707 → E₂ = 27.07
P(3) = 4e⁻²/3 = 0.1804 → E₃ = 18.04
P(≥4) = 1 - 上面全部 = 0.1429 → E₄ = 14.29

χ² = (15-13.53)²/13.53 + (22-27.07)²/27.07
   + (28-27.07)²/27.07 + (20-18.04)²/18.04
   + (15-14.29)²/14.29

   = 0.160 + 0.950 + 0.032 + 0.213 + 0.035
   = 1.390

df = 5 - 1 = 4（5 個類別，無估計參數）
臨界值 (α=0.05, df=4) = 9.488

1.390 < 9.488 → 不拒絕 H₀

結論：數據與 Poisson(λ=2) 一致。
```
</details>

**題目 2**：計算以下假設檢定的 Power。H₀: μ=0, H₁: μ≠0, α=0.05, n=50, 真實 μ=0.5, σ=1。

<details>
<summary>解答</summary>

```
標準誤：SE = σ/√n = 1/√50 = 0.1414
臨界值：z₀.₀₂₅ = 1.96

拒絕域：X̄ > 1.96 × 0.1414 = 0.277 或 X̄ < -0.277

在 H₁ (μ=0.5) 下：
  P(X̄ > 0.277) = P(Z > (0.277-0.5)/0.1414)
                = P(Z > -1.578)
                = 0.943

  P(X̄ < -0.277) = P(Z < (-0.277-0.5)/0.1414)
                 = P(Z < -5.50)
                 ≈ 0

Power ≈ 0.943

β = 1 - 0.943 = 0.057 ≈ 5.7%

這個檢定的 Power 很高（94.3%），
因為效應量 d = 0.5/1 = 0.5 算是中等，
而且 n=50 提供了足夠的統計力。
```
</details>


# 機率教室 第五篇：隨機 — 馬可夫鏈、隨機漫步、Poisson 過程

> 對應小說第 49-60 章。最後一篇。這裡的數學是整本書最進階的——但也是最美的。馬可夫鏈讓你看到系統如何收斂，隨機漫步讓你看到自由如何行走，Poisson 過程讓你看到意外如何降臨。

---

## 1. 馬可夫鏈

**小說出處**：第49章（第二階段）、第50章（七十二小時）

### 定義

馬可夫鏈是一個隨機過程，滿足**馬可夫性質**（無記憶性）：

$$P(X_{n+1} = j \mid X_n = i, X_{n-1}, ..., X_0) = P(X_{n+1} = j \mid X_n = i) = p_{ij}$$

> 下一步只取決於現在，不取決於過去。

### 轉移矩陣

$$P = \begin{pmatrix} p_{11} & p_{12} & \cdots & p_{1m} \\ p_{21} & p_{22} & \cdots & p_{2m} \\ \vdots & & \ddots & \vdots \\ p_{m1} & p_{m2} & \cdots & p_{mm} \end{pmatrix}$$

每一行加起來 = 1（從任何狀態出發，必須到某個地方去）。

### 小說中的設定

```
Oracle Prime 的馬可夫鏈：
  - 152 個狀態（台灣 152 個行政區域）
  - 每個區域的先驗構成一個狀態
  - 轉移矩陣 P 定義了先驗如何從一步更新到下一步
  - 每一步 = 6 小時
  - 柏朗的目標：讓所有區域收斂到他設定的穩態 π
```

### 穩態分布

如果存在 π 使得：

$$\boldsymbol{\pi} = \boldsymbol{\pi} P$$

那 π 就是穩態分布（stationary distribution）。

**計算方法**：π 是 P 的轉置矩陣的特徵值 1 對應的特徵向量。

### 數值範例

```
簡化的 3 × 3 轉移矩陣：

P = | 0.7  0.2  0.1 |
    | 0.15 0.7  0.15|
    | 0.1  0.2  0.7 |

求穩態：解 π = πP

  π₁ = 0.7π₁ + 0.15π₂ + 0.1π₃
  π₂ = 0.2π₁ + 0.7π₂ + 0.2π₃
  π₃ = 0.1π₁ + 0.15π₂ + 0.7π₃
  π₁ + π₂ + π₃ = 1

從第一式：0.3π₁ = 0.15π₂ + 0.1π₃
從第三式：0.3π₃ = 0.1π₁ + 0.15π₂

解聯立：π₁ ≈ 0.326, π₂ ≈ 0.370, π₃ ≈ 0.304

不管你從哪個狀態開始，最終都會收斂到這個分布。
這就是柏朗想達到的——所有人的先驗都被「校準」到他設定的 π。
```

> 「不管你從哪個狀態開始。走夠多步。最終都會到同一個地方。這就是柏朗想要的——所有人的先驗，最終收斂到他設定的那個世界。」——第四十九章

![Markov Chain](圖表/19_markov_chain.png)

---

## 2. 收斂速率與 |λ₂|

**小說出處**：第50章（七十二小時）、第52章（收斂）

### 收斂定理

如果馬可夫鏈是**不可約**（irreducible）且**非周期**（aperiodic）的：

$$\| \boldsymbol{\pi}_n - \boldsymbol{\pi} \| \sim C \cdot |\lambda_2|^n$$

- $|\lambda_2|$：轉移矩陣的第二大特徵值的絕對值
- n：步數
- $|\lambda_2|$ 越小，收斂越快

### 小說中的倒計時

```
柏朗不斷調整轉移矩陣，加速收斂：

  初始：|λ₂| = 0.847
    → 達到不可逆閾值 (0.01) 需要 log(0.01)/log(0.847) ≈ 27.8 步
    → 27.8 步 × 6 小時/步 ≈ 167 小時 ≈ 7 天

  第一次調整：|λ₂| = 0.793
    → log(0.01)/log(0.793) ≈ 19.8 步 ≈ 119 小時 ≈ 5 天

  第二次調整：|λ₂| = 0.761
    → log(0.01)/log(0.761) ≈ 16.9 步 ≈ 101 小時 ≈ 4.2 天

  柏朗在壓縮時間。每次調整 |λ₂|，他都在加快世界走向「穩態」的速度。

收斂到第九步（Ch.56）：
  ‖π_n - π‖ = 0.019
  不可逆閾值 = 0.010
  差距 = 0.009

  再一步就不可逆了。
  蘇立恆在這一步之前凍結了馬可夫引擎。
```

### 數值計算

```
要多少步 n 才能讓 |λ₂|ⁿ < ε？

n > log(ε) / log(|λ₂|)

|λ₂| = 0.847, ε = 0.001（完全穩態）：
  n > log(0.001) / log(0.847) = -6.908 / -0.166 = 41.6 步

|λ₂| = 0.761, ε = 0.001：
  n > log(0.001) / log(0.761) = -6.908 / -0.273 = 25.3 步

省了 16 步 → 省了 96 小時 → 四天。
```

---

## 3. 遍歷性定理

**小說出處**：第50章

### 定理

如果馬可夫鏈是不可約、非周期、正常返的：

$$\frac{1}{n} \sum_{k=0}^{n-1} \mathbf{1}(X_k = i) \xrightarrow{n \to \infty} \pi_i$$

> 在狀態 i 花費的時間比例，長期來看等於穩態機率 π_i。

### 直覺

想像你在一座城市裡隨機走路。雖然你的路線是隨機的，但長期來看，你在每個區域花的時間比例是固定的——等於穩態分布。

你不需要知道穩態是什麼。走夠久了，你自然會花「正確」的時間在每個地方。

### 小說的意義

```
柏朗想利用遍歷定理：
  只要讓馬可夫鏈跑夠多步，
  每個區域的先驗就會自然收斂到他設定的值。
  不需要直接修改。只需要設好轉移矩陣。讓時間做剩下的事。

遍歷性 = 不可逆。一旦收斂，就永遠是那個狀態。
除非你凍結馬可夫引擎——在它收斂之前。
```

> 「遍歷性。一旦收斂，就永遠是那個狀態。除非你在它收斂之前——凍結一切。」——第五十章，馬可瑜的計算

![Ergodic Theorem](圖表/23_ergodic_theorem.png)

---

## 4. 隨機漫步 (Random Walk)

**小說出處**：第51章（擲硬幣）、第52章（收斂）

### 一維隨機漫步

每一步：

$$X_n = X_{n-1} + Z_n, \quad Z_n = \begin{cases} +1 & \text{with prob } p \\ -1 & \text{with prob } 1-p \end{cases}$$

位置 = 所有步驟的總和：$S_n = \sum_{i=1}^{n} Z_i$

### 公平漫步 (p = 0.5)

- $E[S_n] = 0$（平均不動）
- $\text{Var}(S_n) = n$
- $\sigma(S_n) = \sqrt{n}$（偏離零點的期望距離以 √n 增長）

**回歸定理**：一維公平漫步以機率 1 回到原點。
但二維公平漫步也以機率 1 回到原點。
三維以上的公平漫步——不一定回得來。

### 偏幣漫步 (p = 0.55)——小說的設定

```
蘇立恆的硬幣：P(正面) = 0.55, P(反面) = 0.45

漂移 = E[Zn] = 0.55 × (+1) + 0.45 × (-1) = 0.10

每一步平均往目標方向移動 0.10

在 n 步後：
  E[Sn] = 0.10n（線性漂移）
  Var(Sn) = n × 0.55 × 0.45 × 4 = 0.99n
  σ(Sn) = 0.995√n

第52章數據：
  47 步走完
  左轉（朝目標）：28 次 (59.6%)
  右轉：19 次 (40.4%)

  期望：55% 左轉 = 25.85 次
  觀測：28 次 → 比期望多

  直線距離：7.3 公里
  實際路程：14.2 公里
  效率：7.3 / 14.2 = 51.4%

  效率 51.4% 意味著一半的路程被「隨機」消耗了。
  但正是這個消耗讓他們不可預測。
```

### 偏幣漫步 vs 馬可夫預測

```
為什麼偏幣能打敗馬可夫鏈的預測？

馬可夫鏈的預測依賴轉移矩陣——
它假設你的下一步只取決於當前狀態。

但如果你每一步的方向由硬幣決定——
轉移矩陣無法預測硬幣的結果。

每一步都有 45% 的機率「錯誤方向」。
這個 45% 的噪音就是你的護甲。
馬可夫鏈可以預測你的平均趨勢——但無法預測單步。

「轉移矩陣能算出你大概往哪走。但它不知道你下一步往哪走。」
```

> 「你連擲硬幣都這麼嚴謹。」「實驗設計的基本功。隨機分配。雙盲。不讓偏好進來。」——第五十一章，貝葉與蘇立恆在路口

> 「效率百分之五十一。意味著一半的路程被隨機消耗了。但正是這一半的消耗讓他們不可預測。」——第五十二章

![Random Walk](圖表/20_random_walk.png)
![2D Random Walk](圖表/21_2d_random_walk.png)

---

## 5. Poisson 過程

**小說出處**：第53章（Poisson 的意外）

### 定義

Poisson 過程 {N(t), t ≥ 0} 是一個計數過程，滿足：

1. N(0) = 0
2. 增量獨立
3. 在 [t, t+s) 內發生的事件數 ~ Poisson(λs)

$$P(N(t+s) - N(t) = k) = \frac{(\lambda s)^k e^{-\lambda s}}{k!}$$

### 等待時間——指數分布

第一個事件的等待時間 T ~ Exponential(λ)：

$$P(T > t) = e^{-\lambda t}$$

$$E[T] = \frac{1}{\lambda}$$

### 無記憶性

$$P(T > t + s \mid T > t) = P(T > s)$$

> 你已經等了多久完全不影響接下來還要等多久。

### 小說中的數值（第53章）

```
λ = 0.58 事件/小時

P(0 events in 1 hour) = e^(-0.58) = 0.560

  → 56% 的機率下一個小時什麼都不會發生

P(≥1 event in 1 hour) = 1 - 0.560 = 0.440

  → 44% 的機率至少有一件意外

期望等待時間 = 1/0.58 ≈ 1.72 小時

過去 12 小時的數據：
  7 次 Poisson 事件
  正向 3 次，負向 4 次
  平均間隔 12/7 ≈ 1.71 小時（非常接近理論值 1.72）

「Poisson 事件不可預測。但它的頻率可以計算。
你不知道下一個意外什麼時候來。
但你知道——平均每 1.72 小時一個。
這就是隨機的秩序。」
```

### Poisson 過程的美妙性質

1. **合併**：兩個獨立的 Poisson(λ₁) 和 Poisson(λ₂) 合併 → Poisson(λ₁ + λ₂)
2. **分裂**：每個事件獨立分類為 A (機率 p) 或 B (機率 1-p) → A 是 Poisson(λp)，B 是 Poisson(λ(1-p))
3. **均勻分佈**：給定 N(t) = n，n 個事件的發生時間在 [0, t] 上均勻分佈

> 「你不知道下一個意外什麼時候來。但你知道——平均每 1.72 小時一個。這就是隨機的秩序。不確定性是代價。但不確定性也是武器。」——第五十三章

![Poisson Process](圖表/22_poisson_process.png)

---

## 6. Graceful Shutdown vs. 強制關閉

**小說出處**：第54章（消防梯）、第56章（S.L.H.）

這不是純數學概念，但背後的邏輯是：

```
馬可夫鏈收斂中（第九步，共十二步）：
  ‖π_n - π‖ = 0.019
  不可逆閾值 = 0.010

兩種選擇：

方案A：Graceful Shutdown（優雅關閉）
  - 讓馬可夫引擎完成當前步驟
  - 停止轉移矩陣
  - 所有區域的先驗凍結在當前值
  - 然後逐步回退到修改前的值
  - 需要管理員權限（卡片）

方案B：Emergency Freeze（緊急凍結）
  - 直接斷電
  - 馬可夫鏈在轉移過程中被中斷
  - 部分區域在舊值，部分在新值
  - 先驗不一致 → 系統崩潰

蘇立恆選了方案A——但需要柏朗的卡片。
這就是天台對話的數學背景。
```

---

## 7. v5.0 的選擇分布 — 自由的數學

**小說出處**：第57章（選擇）

### 結果

| 選項 | 比例 | 數學含義 |
|------|------|----------|
| 官方先驗 | 33.1% | 繼續使用 Oracular 的貝氏更新 |
| 個人先驗 | 29.4% | 使用自己的先驗模型 |
| 無先驗模式 | 16.8% | 只看似然，不加先驗 |
| 關閉透鏡 | 9.2% | 完全不使用系統 |
| 尚未決定 | 11.5% | 還在思考 |

### 為什麼「沒有超過 50%」很重要？

```
馬可瑜的分析：

「沒有任何一個選項超過百分之五十。」

如果有一個選項 > 80%：
  → 可能有外力在推（宣傳、恐懼、從眾效應）
  → 選擇不是自由的

但 33% / 29% / 17% / 9% / 12%：
  → 每個選項都有人選
  → 選擇是分散的
  → 這就是自由分布的樣子

人類本來就不一樣。
先驗本來就每個人不同。
一個真正自由的選擇——看起來就是這樣。
分散的。不整齊的。充滿方差的。
```

> 「沒有任何一個選項超過百分之五十。這就是自由分布的樣子。人類本來就不一樣。先驗本來就每個人不同。這是對的。」——第五十七章，馬可瑜看著三個螢幕

![Prior Choice Distribution](圖表/24_prior_choice.png)

---

## 練習題

**題目 1**：給定轉移矩陣 P = [[0.8, 0.2], [0.4, 0.6]]，求穩態分布 π。

<details>
<summary>解答</summary>

```
π = πP
π₁ = 0.8π₁ + 0.4π₂
π₂ = 0.2π₁ + 0.6π₂
π₁ + π₂ = 1

從第一式：0.2π₁ = 0.4π₂ → π₁ = 2π₂
代入 π₁ + π₂ = 1：2π₂ + π₂ = 1 → π₂ = 1/3

π = (2/3, 1/3)

驗證：π P = (2/3, 1/3) × [[0.8, 0.2], [0.4, 0.6]]
     = (2/3 × 0.8 + 1/3 × 0.4, 2/3 × 0.2 + 1/3 × 0.6)
     = (0.533 + 0.133, 0.133 + 0.200)
     = (0.667, 0.333)
     = (2/3, 1/3) ✓
```
</details>

**題目 2**：|λ₂| = 0.85。要讓 ‖π_n - π‖ < 0.001，至少需要多少步？

<details>
<summary>解答</summary>

```
|λ₂|ⁿ < 0.001
0.85ⁿ < 0.001
n × log(0.85) < log(0.001)
n > log(0.001) / log(0.85)
n > -6.908 / -0.163
n > 42.4

至少需要 43 步。

如果每步 6 小時：43 × 6 = 258 小時 ≈ 10.75 天

如果柏朗把 |λ₂| 壓到 0.76：
  n > -6.908 / -0.274 = 25.2 → 26 步
  26 × 6 = 156 小時 ≈ 6.5 天

省了 4.25 天。這就是「加速收斂」的實際效果。
```
</details>

**題目 3**：顧客到達商店服從 Poisson(λ=3/hr)。求：(a) 一小時內剛好 5 位顧客的機率 (b) 等待下一位顧客超過 30 分鐘的機率

<details>
<summary>解答</summary>

```
(a) P(N(1) = 5) = e^(-3) × 3⁵ / 5!
   = 0.0498 × 243 / 120
   = 12.102 / 120
   = 0.1008 ≈ 10.1%

(b) 等待時間 T ~ Exp(λ=3)
   30 分鐘 = 0.5 小時

   P(T > 0.5) = e^(-3 × 0.5)
              = e^(-1.5)
              = 0.2231 ≈ 22.3%

   無記憶性驗證：
   假設你已經等了 10 分鐘還沒人來。
   P(再等 20 分鐘以上 | 已等 10 分鐘)
   = P(T > 30min | T > 10min)
   = P(T > 20min)  ← 無記憶性！
   = e^(-3 × 1/3)
   = e^(-1) = 0.368

   過去等的 10 分鐘完全不影響未來。
   這就是指數分布的無記憶性。
```
</details>

---

## 全書公式速查表

| 名稱 | 公式 | 出處 |
|------|------|------|
| 貝氏定理 | P(θ\|D) ∝ P(θ) × P(D\|θ) | Ch.11 |
| Beta 期望值 | E[θ] = α/(α+β) | Ch.1 |
| Beta 共軛更新 | Beta(α+k, β+n-k) | Ch.16 |
| Poisson PMF | P(X=k) = e^(-λ)λᵏ/k! | Ch.4, 53 |
| 指數 CDF | P(T≤t) = 1-e^(-λt) | Ch.53 |
| 期望值線性性 | E[aX+b] = aE[X]+b | Ch.25 |
| 變異數 | Var(X) = E[X²]-(E[X])² | Ch.27 |
| Chebyshev | P(\|X-μ\|≥kσ) ≤ 1/k² | Ch.29 |
| MGF | M(t) = E[e^(tX)] | Ch.34 |
| χ² 檢定 | χ² = Σ(O-E)²/E | Ch.44 |
| 穩態分布 | π = πP | Ch.49 |
| 收斂速率 | ‖π_n-π‖ ~ \|λ₂\|ⁿ | Ch.50 |
| Random Walk 漂移 | E[Sn] = n(2p-1) | Ch.51 |
| Poisson 過程 | N(t) ~ Poisson(λt) | Ch.53 |
